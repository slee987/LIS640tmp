{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lis640_week09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slee987/LIS640tmp/blob/main/lis640_week09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI1t86uZAqpj"
      },
      "source": [
        "## 1 Set up Environment in Google Colab\n",
        "\n",
        "Run the following cells to install/upgrade the required packages and check if the installed versions meet the requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA2P2alFAt8d"
      },
      "source": [
        "# Upgrade packages in Google Colab.\n",
        "\n",
        "# upgrade pip\n",
        "!pip3 install pip --upgrade\n",
        "\n",
        "# upgrade spacy 3.0.x\n",
        "!pip3 install 'spacy>=3.0.0,<3.1.0' --upgrade\n",
        "\n",
        "# upgrade nltk 3.5\n",
        "!pip3 install 'nltk>=3.5,<3.6' --upgrade\n",
        "\n",
        "# upgrade scikit-learn 0.24.x\n",
        "!pip3 install 'scikit-learn>=0.24.0,<0.25.0' --upgrade\n",
        "\n",
        "# download the latest spacy model\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scaJCgBVA1kz"
      },
      "source": [
        "# Check the installed versions.\n",
        "# We require: spacy == 3.0.x ; nltk == 3.5.x ; scikit-learn == 0.24.x ; spacy's en-core-web-sm == 3.0.x .\n",
        "\n",
        "import re\n",
        "\n",
        "pkgs = !pip3 list\n",
        "versions = { pkg.split()[0]:pkg.split()[1] for pkg in pkgs if re.match( '(spacy\\s.+)|(nltk\\s.+)|(scikit-learn\\s.+)|(en-core-web-sm\\s.+)', pkg ) }\n",
        "\n",
        "assert versions['spacy'][0:3] == '3.0'\n",
        "assert versions['nltk'][0:3] == '3.5'\n",
        "assert versions['scikit-learn'][0:4] == '0.24'\n",
        "assert versions['en-core-web-sm'][0:3] == '3.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsMJNcX4QLAR"
      },
      "source": [
        "## 2 Explore the Dataset by Pang et al. (2002)\n",
        "\n",
        "Download the raw dataset at:\n",
        "http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFGRjltDNH4O"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load( \"en_core_web_sm\", disable=[\"parser\", \"ner\"] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o1FLt-fNH6q"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv( 'https://jiepujiang.github.io/data/pang2002.csv', index_col=0 )\n",
        "# data = data[650:750]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1iuOotSXOR"
      },
      "source": [
        "## 3 Implement Pang et al. (2002)'s methods\n",
        "\n",
        "https://www.cs.cornell.edu/home/llee/papers/sentiment.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw2Ty-5pSj80"
      },
      "source": [
        "# Extract ungiram features (considering word frequencies): row (1)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import feature_selection\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def count_unigram_freq( rawtext, nlp ):\n",
        "    return [ token.lemma_.lower() for token in nlp(rawtext) if not token.is_stop and not token.is_punct ]\n",
        "\n",
        "def train_feat_unigram_freq( data, nlp, top_feats ):\n",
        "\n",
        "    dict_unigram = DictVectorizer()\n",
        "    X = dict_unigram.fit_transform( Counter(count_unigram_freq(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "\n",
        "    chi2, pval = feature_selection.chi2( X, Y )\n",
        "    top = min( top_feats, X.shape[1] )\n",
        "    fsel = feature_selection.SelectKBest( score_func = feature_selection.chi2, k = top )\n",
        "    X_selected = fsel.fit_transform( X, Y )\n",
        "    dict_selected = dict_unigram.restrict( fsel.get_support() )\n",
        "\n",
        "    return X_selected, Y, dict_selected\n",
        "\n",
        "def test_feat_unigram_freq( data, nlp, dict_unigram ):\n",
        "    X = dict_unigram.transform( Counter(count_unigram_freq(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLLtQ0pbbB9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n4dN_oXWX0o"
      },
      "source": [
        "# Extract ungiram features (considering word occurrences only): row (2)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import feature_selection\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def count_unigram_occur( rawtext, nlp ):\n",
        "    return set([ token.lemma_.lower() for token in nlp(rawtext) if not token.is_stop and not token.is_punct ])\n",
        "\n",
        "def train_feat_unigram_occur( data, nlp, top_feats ):\n",
        "\n",
        "    dict_unigram = DictVectorizer()\n",
        "    X = dict_unigram.fit_transform( Counter(count_unigram_occur(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "\n",
        "    chi2, pval = feature_selection.chi2( X, Y )\n",
        "    top = min( top_feats, X.shape[1] )\n",
        "    fsel = feature_selection.SelectKBest( score_func = feature_selection.chi2, k = top )\n",
        "    X_selected = fsel.fit_transform( X, Y )\n",
        "    dict_selected = dict_unigram.restrict( fsel.get_support() )\n",
        "\n",
        "    return X_selected, Y, dict_selected\n",
        "\n",
        "def test_feat_unigram_occur( data, nlp, dict_unigram ):\n",
        "    X = dict_unigram.transform( Counter(count_unigram_occur(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9LBVofXNNw"
      },
      "source": [
        "# Extract ungiram+bigram features (considering word occurrences only): row (3)\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import feature_selection\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def count_bigram_occur( rawtext, nlp ):\n",
        "    unigrams_withoov = [ '[OOV]' if token.is_stop or token.is_punct else token.lemma_.lower() for token in nlp(rawtext) ]\n",
        "    return set([ unigrams_withoov[i]+'_'+unigrams_withoov[i+1] for i in range(len(unigrams_withoov)-1) if unigrams_withoov[i]!='[OOV]' and unigrams_withoov[i+1]!='[OOV]' ])\n",
        "\n",
        "def train_feat_unibi_occur( data, nlp, top_feats ):\n",
        "\n",
        "    dict_unigram = DictVectorizer()\n",
        "    dict_bigram = DictVectorizer()\n",
        "    X_unigram = dict_unigram.fit_transform( Counter(count_unigram_occur(text, nlp)) for text in data['text'] )\n",
        "    X_bigram = dict_bigram.fit_transform( Counter(count_bigram_occur(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "\n",
        "    chi2_unigram, pval_unigram = feature_selection.chi2( X_unigram, Y )\n",
        "    top_unigram = min( top_feats, X_unigram.shape[1] )\n",
        "    fsel_unigram = feature_selection.SelectKBest( score_func = feature_selection.chi2, k = top_unigram )\n",
        "    X_unigram_selected = fsel_unigram.fit_transform( X_unigram, Y )\n",
        "    dict_unigram_selected = dict_unigram.restrict( fsel_unigram.get_support() )\n",
        "\n",
        "    chi2_bigram, pval_bigram = feature_selection.chi2( X_bigram, Y )\n",
        "    top_bigram = min( top_feats, X_bigram.shape[1] )\n",
        "    fsel_bigram = feature_selection.SelectKBest( score_func = feature_selection.chi2, k = top_bigram )\n",
        "    X_bigram_selected = fsel_bigram.fit_transform( X_bigram, Y )\n",
        "    dict_bigram_selected = dict_bigram.restrict( fsel_bigram.get_support() )\n",
        "\n",
        "    return hstack((X_unigram_selected, X_bigram_selected)), Y, dict_unigram_selected, dict_bigram_selected\n",
        "\n",
        "def test_feat_unibi_occur( data, nlp, dict_unigram, dict_bigram ):\n",
        "    X_unigram = dict_unigram.transform( Counter(count_unigram_occur(text, nlp)) for text in data['text'] )\n",
        "    X_bigram = dict_bigram.transform( Counter(count_bigram_occur(text, nlp)) for text in data['text'] )\n",
        "    Y = np.array(data['label'])\n",
        "    return hstack((X_unigram, X_bigram)), Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFq00dzavtx"
      },
      "source": [
        "## 4 Set Up Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHnKP9dwapuO"
      },
      "source": [
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "def eval_nb_unigram_freq( data_train, data_test, top_feats ):\n",
        "    X_train, Y_train, dict_unigram = train_feat_unigram_freq( data_train, nlp, top_feats )\n",
        "    X_test, Y_test = test_feat_unigram_freq( data_test, nlp, dict_unigram )\n",
        "    classifier = sklearn.naive_bayes.MultinomialNB()\n",
        "    classifier.fit( X_train, Y_train )\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    return sklearn.metrics.precision_recall_fscore_support( Y_test, Y_pred, labels=['pos', 'neg'] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufopHRQWapw6"
      },
      "source": [
        "\n",
        "metrics = eval_nb_unigram_freq( data[data['fold']!=1], data[data['fold']==1], 1000 )\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKMhjXzMc_Xr"
      },
      "source": [
        "## 5 In-class Exercise: Cross-Validation\n",
        "\n",
        "train[1,2]->test[3]\n",
        "\n",
        "train[1,3]->test[2]\n",
        "\n",
        "train[2,3]->test[1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce5Hoh5Hap5J"
      },
      "source": [
        "# your solution here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-82FkE2COPxK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCkigzfSNIAH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}