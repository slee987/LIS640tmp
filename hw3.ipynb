{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slee987/LIS640tmp/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI1t86uZAqpj"
      },
      "source": [
        "## 1 Install Packages if Needed\n",
        "\n",
        "Run the following cells to install/upgrade the required packages and check if the installed versions meet the requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA2P2alFAt8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41632b2a-9e63-4cef-e394-42389503d8e9"
      },
      "source": [
        "# Upgrade packages in Google Colab.\n",
        "\n",
        "# upgrade pip\n",
        "!pip3 install pip --upgrade\n",
        "\n",
        "# upgrade spacy 3.0.x\n",
        "!pip3 install 'spacy>=3.0.0,<3.1.0' --upgrade\n",
        "\n",
        "# upgrade scikit-learn 0.24.x\n",
        "!pip3 install 'scikit-learn>=0.24.0,<0.25.0' --upgrade\n",
        "\n",
        "# download the latest spacy model\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 12.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Collecting spacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 308 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (20.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (2.11.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy_legacy-3.0.2-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "  Downloading thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (54.2.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (1.0.5)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.4.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (2.0.5)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (3.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (4.41.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (1.19.5)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "  Downloading srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0) (2.4.7)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0) (1.1.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=733bbc9086fee12c00903d8a4b0be4ff585c9c07be83bf481fc94de482e2de81\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
            "2021-04-04 03:39:14.237413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scaJCgBVA1kz"
      },
      "source": [
        "# Check the installed versions.\n",
        "# We require: spacy == 3.0.x ; scikit-learn == 0.24.x ; spacy's en-core-web-sm == 3.0.x .\n",
        "\n",
        "import re\n",
        "\n",
        "pkgs = !pip3 list\n",
        "versions = { pkg.split()[0]:pkg.split()[1] for pkg in pkgs if re.match( '(spacy\\s.+)|(scikit-learn\\s.+)|(en-core-web-sm\\s.+)', pkg ) }\n",
        "\n",
        "assert versions['spacy'][0:3] == '3.0'\n",
        "assert versions['scikit-learn'][0:4] == '0.24'\n",
        "assert versions['en-core-web-sm'][0:3] == '3.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsMJNcX4QLAR"
      },
      "source": [
        "## 2 Feature Extraction for Unigrams and Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFGRjltDNH4O"
      },
      "source": [
        "import spacy\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load( \"en_core_web_sm\", disable=[\"parser\", \"ner\"] )\n",
        "\n",
        "# Transform a rawtext into a list of unigrams using spacy. Punctuations and stop words will be replaced by a [OOV] token.\n",
        "def text2unigrams( rawtext, nlp ):\n",
        "    return [ '[OOV]' if token.is_stop or token.is_punct else token.lemma_.lower() for token in nlp(rawtext) ]\n",
        "\n",
        "# Generate a list of bigrams from a list of unigrams. We require none of the tokens in bigrams are [OOV].\n",
        "def unigrams2bigrams( unigrams ):\n",
        "    return [ unigrams[i]+'_'+unigrams[i+1] for i in range(len(unigrams)-1) if unigrams[i]!='[OOV]' and unigrams[i+1]!='[OOV]' ]\n",
        "\n",
        "# Count the frequency of a list features.\n",
        "def count_freq( features ):\n",
        "    counts = Counter( features )\n",
        "    counts.pop('[OOV]', None)\n",
        "    return counts\n",
        "\n",
        "# Count the presence of a list features.\n",
        "def count_occur( features ):\n",
        "    counts = Counter( set( features ) )\n",
        "    counts.pop('[OOV]', None)\n",
        "    return counts\n",
        "\n",
        "# Generate readable evaluation results.\n",
        "def readable_eval( Y_test, Y_pred, labels ):\n",
        "    prf = sklearn.metrics.precision_recall_fscore_support( Y_test, Y_pred, labels=labels )\n",
        "    metrics = {}\n",
        "    metrics.update( { 'Precision (%s)'%label:prec for (prec,label) in zip(prf[0], labels) } )\n",
        "    metrics.update( { 'Recall (%s)'%label:rec for (rec,label) in zip(prf[1], labels) } )\n",
        "    metrics.update( { 'F1 (%s)'%label:f1 for (f1,label) in zip(prf[2], labels) } )\n",
        "    metrics['Accuracy'] = sklearn.metrics.accuracy_score( Y_test, Y_pred )\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk5UdPFWlhU3"
      },
      "source": [
        "## 3 Load and Preprocess Pang et al. (2002)'s dataset.\n",
        "\n",
        "Note that the following step will take 1-2 minutes. You are encouraged to reuse data['unigrams'] and data['bigrams'] instead of preprocess the texts multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o1FLt-fNH6q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9c3e633a-21ef-4358-9f70-44d1e052f871"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv( 'https://jiepujiang.github.io/data/pang2002.csv', index_col=0 )\n",
        "\n",
        "data['unigrams'] = [ text2unigrams(text, nlp) for text in data['text'] ]\n",
        "data['bigrams'] = [ unigrams2bigrams(unigrams) for unigrams in data['unigrams'] ]\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>unigrams</th>\n",
              "      <th>bigrams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cv004_tok-29856.txt</th>\n",
              "      <td>1</td>\n",
              "      <td>pos</td>\n",
              "      <td>all great things come to an end , and the dot-...</td>\n",
              "      <td>[[OOV], great, thing, come, [OOV], [OOV], end,...</td>\n",
              "      <td>[great_thing, thing_come, com_era, era_embodie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv409_tok-11193.txt</th>\n",
              "      <td>2</td>\n",
              "      <td>pos</td>\n",
              "      <td>i'm not quite sure how best to go about writin...</td>\n",
              "      <td>[[OOV], [OOV], [OOV], [OOV], sure, [OOV], good...</td>\n",
              "      <td>[little_disappointed, barry_levinson, politica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv045_tok-29121.txt</th>\n",
              "      <td>1</td>\n",
              "      <td>pos</td>\n",
              "      <td>the others ( 2001 ) nicole kidman , christophe...</td>\n",
              "      <td>[[OOV], [OOV], [OOV], 2001, [OOV], nicole, kid...</td>\n",
              "      <td>[nicole_kidman, christopher_eccleston, fionnul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv279_tok-15969.txt</th>\n",
              "      <td>2</td>\n",
              "      <td>pos</td>\n",
              "      <td>director : tony scott writer : david marconi s...</td>\n",
              "      <td>[director, [OOV], tony, scott, writer, [OOV], ...</td>\n",
              "      <td>[tony_scott, scott_writer, david_marconi, marc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv387_tok-4672.txt</th>\n",
              "      <td>2</td>\n",
              "      <td>pos</td>\n",
              "      <td>one of the most entertaining james bond films ...</td>\n",
              "      <td>[[OOV], [OOV], [OOV], [OOV], entertaining, jam...</td>\n",
              "      <td>[entertaining_jame, jame_bond, bond_film, roge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv562_tok-26379.txt</th>\n",
              "      <td>3</td>\n",
              "      <td>neg</td>\n",
              "      <td>directed by : jan de bont written by : david s...</td>\n",
              "      <td>[direct, [OOV], [OOV], jan, de, bont, write, [...</td>\n",
              "      <td>[jan_de, de_bont, bont_write, david_shelf, shi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv000_tok-9611.txt</th>\n",
              "      <td>1</td>\n",
              "      <td>neg</td>\n",
              "      <td>tristar / 1 : 30 / 1997 / r ( language , viole...</td>\n",
              "      <td>[tristar, [OOV], 1, [OOV], 30, [OOV], 1997, [O...</td>\n",
              "      <td>[dennis_rodman, claude_van, van_damme, mickey_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv571_tok-11568.txt</th>\n",
              "      <td>3</td>\n",
              "      <td>neg</td>\n",
              "      <td>director : michael caton-jones writer : chuck ...</td>\n",
              "      <td>[director, [OOV], michael, caton, [OOV], jones...</td>\n",
              "      <td>[michael_caton, jones_writer, chuck_pfarrer, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv210_tok-15092.txt</th>\n",
              "      <td>1</td>\n",
              "      <td>neg</td>\n",
              "      <td>wrongfully accused reviewed by jamie peck&lt;hr&gt;r...</td>\n",
              "      <td>[wrongfully, accuse, review, [OOV], jamie, pec...</td>\n",
              "      <td>[wrongfully_accuse, accuse_review, jamie_peck,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cv629_tok-26880.txt</th>\n",
              "      <td>3</td>\n",
              "      <td>neg</td>\n",
              "      <td>throw some shots of a vaguely menacing fetus o...</td>\n",
              "      <td>[throw, [OOV], shot, [OOV], [OOV], vaguely, me...</td>\n",
              "      <td>[vaguely_menacing, menacing_fetus, opening_cre...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     fold  ...                                            bigrams\n",
              "cv004_tok-29856.txt     1  ...  [great_thing, thing_come, com_era, era_embodie...\n",
              "cv409_tok-11193.txt     2  ...  [little_disappointed, barry_levinson, politica...\n",
              "cv045_tok-29121.txt     1  ...  [nicole_kidman, christopher_eccleston, fionnul...\n",
              "cv279_tok-15969.txt     2  ...  [tony_scott, scott_writer, david_marconi, marc...\n",
              "cv387_tok-4672.txt      2  ...  [entertaining_jame, jame_bond, bond_film, roge...\n",
              "...                   ...  ...                                                ...\n",
              "cv562_tok-26379.txt     3  ...  [jan_de, de_bont, bont_write, david_shelf, shi...\n",
              "cv000_tok-9611.txt      1  ...  [dennis_rodman, claude_van, van_damme, mickey_...\n",
              "cv571_tok-11568.txt     3  ...  [michael_caton, jones_writer, chuck_pfarrer, k...\n",
              "cv210_tok-15092.txt     1  ...  [wrongfully_accuse, accuse_review, jamie_peck,...\n",
              "cv629_tok-26880.txt     3  ...  [vaguely_menacing, menacing_fetus, opening_cre...\n",
              "\n",
              "[1400 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1iuOotSXOR"
      },
      "source": [
        "## 4 Prepare Features for Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ZlLV4WSHsm"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import feature_selection\n",
        "from collections import Counter\n",
        "\n",
        "# Extract and select unigram or bigram features on a specific set of data.\n",
        "# count_func: either count_freq or count_occur, used for indicating considering feature freq. or pres.\n",
        "# data: the set of data to extract and select features (a pandas dataframe)\n",
        "# col_feats: the name of the column in the data to extract features ('unigrams' or 'bigrams')\n",
        "# top_feats: number of top features to select using Chi-squared\n",
        "def train_select_feat( count_func, data, col_feats, top_feats ):\n",
        "\n",
        "    dictvec = DictVectorizer()\n",
        "    X = dictvec.fit_transform( count_func(tokens) for tokens in data[col_feats] )\n",
        "    Y = np.array(data['label'])\n",
        "\n",
        "    chi2, pval = feature_selection.chi2( X, Y )\n",
        "    top = min( top_feats, X.shape[1] )\n",
        "    fsel = feature_selection.SelectKBest( score_func = feature_selection.chi2, k = top )\n",
        "    X_selected = fsel.fit_transform( X, Y )\n",
        "    dictvec_selected = dictvec.restrict( fsel.get_support() )\n",
        "\n",
        "    return X_selected, Y, dictvec_selected\n",
        "\n",
        "# Extract unigram or bigram features on a specific set of data using a provided set of features.\n",
        "# count_func: either count_freq or count_occur, used for indicating considering feature freq. or pres.\n",
        "# data: the set of data to extract features (a pandas dataframe)\n",
        "# col_feats: the name of the column in the data to extract features ('unigrams' or 'bigrams')\n",
        "# dictvec: a set of features (a DictVectorizer)\n",
        "def test_feat( count_func, data, col_feats, dictvec ):\n",
        "    X = dictvec.transform( count_func(tokens) for tokens in data[col_feats] )\n",
        "    Y = np.array(data['label'])\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9LBVofXNNw"
      },
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Extract and select both unigram and bigram features on a specific set of data.\n",
        "# count_func: either count_freq or count_occur, used for indicating considering feature freq. or pres.\n",
        "# data: the set of data to extract and select features (a pandas dataframe)\n",
        "# top_feats: number of top features to select using Chi-squared\n",
        "def train_select_feat_unibi( count_func, data, top_feats ):\n",
        "    X_uni, Y, dictvec_uni = train_select_feat( count_func, data, 'unigrams', top_feats )\n",
        "    X_bi, Y, dictvec_bi = train_select_feat( count_func, data, 'bigrams', top_feats )\n",
        "    return hstack((X_uni, X_bi)), Y, dictvec_uni, dictvec_bi\n",
        "\n",
        "# Extract both unigram and bigram features on a specific set of data using a provided set of features.\n",
        "# count_func: either count_freq or count_occur, used for indicating considering feature freq. or pres.\n",
        "# data: the set of data to extract features (a pandas dataframe)\n",
        "# dictvec_uni: a set of unigram features (a DictVectorizer)\n",
        "# dictvec_bi: a set of bigram features (a DictVectorizer)\n",
        "def test_feat_unibi( count_func, data, dictvec_uni, dictvec_bi ):\n",
        "    X_uni, Y = test_feat( count_func, data, 'unigrams', dictvec_uni )\n",
        "    X_bi, Y = test_feat( count_func, data, 'bigrams', dictvec_bi )\n",
        "    return hstack((X_uni, X_bi)), Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFq00dzavtx"
      },
      "source": [
        "## Example 1: Train and Evaluate a Unigram Model (considering frequency)\n",
        "\n",
        "Training Set: fold 2 & fold 3; Test Set: fold 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGWY3mzkpGIs",
        "outputId": "3ffb3b93-8016-40d5-eaac-110f7c3414c2"
      },
      "source": [
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "data_train = data[data['fold']!=1]\n",
        "data_test = data[data['fold']==1]\n",
        "\n",
        "X_train, Y_train, dictvec = train_select_feat( count_freq, data_train, 'unigrams', 16165 )\n",
        "X_test, Y_test = test_feat( count_freq, data_test, 'unigrams', dictvec )\n",
        "classifier = sklearn.naive_bayes.MultinomialNB()\n",
        "classifier.fit( X_train, Y_train )\n",
        "Y_pred = classifier.predict(X_test)\n",
        "readable_eval( Y_test, Y_pred, ['pos', 'neg'] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.7532188841201717,\n",
              " 'F1 (neg)': 0.7547974413646056,\n",
              " 'F1 (pos)': 0.751619870410367,\n",
              " 'Precision (neg)': 0.75,\n",
              " 'Precision (pos)': 0.7565217391304347,\n",
              " 'Recall (neg)': 0.759656652360515,\n",
              " 'Recall (pos)': 0.7467811158798283}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcKAg-U_quJJ"
      },
      "source": [
        "## Example 2: Train and Evaluate a Unigram+Bigram Model (considering presence)\n",
        "\n",
        "Training Set: fold 2 & fold 3; Test Set: fold 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urPkDZ8_qWY-",
        "outputId": "dd4997ec-646d-4feb-a87c-a5571ac098e4"
      },
      "source": [
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "data_train = data[data['fold']!=1]\n",
        "data_test = data[data['fold']==1]\n",
        "\n",
        "X_train, Y_train, dictvec_uni, dictvec_bi = train_select_feat_unibi( count_occur, data_train, 16165 )\n",
        "X_test, Y_test = test_feat_unibi( count_occur, data_test, dictvec_uni, dictvec_bi )\n",
        "classifier = sklearn.naive_bayes.MultinomialNB()\n",
        "classifier.fit( X_train, Y_train )\n",
        "Y_pred = classifier.predict(X_test)\n",
        "readable_eval( Y_test, Y_pred, ['pos', 'neg'] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.778969957081545,\n",
              " 'F1 (neg)': 0.7621247113163971,\n",
              " 'F1 (pos)': 0.7935871743486974,\n",
              " 'Precision (neg)': 0.825,\n",
              " 'Precision (pos)': 0.7443609022556391,\n",
              " 'Recall (neg)': 0.7081545064377682,\n",
              " 'Recall (pos)': 0.8497854077253219}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}